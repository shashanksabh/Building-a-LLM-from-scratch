{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a633a526-fa0a-4f8b-82ee-2bc9c0c990c3",
   "metadata": {},
   "source": [
    "## **Building a LLM from Scratch on DGX Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8a16dad-883e-4159-8ed1-c90e211c21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf54be1-1ee3-466a-8d2b-82a577bcfb90",
   "metadata": {},
   "source": [
    "## Multi-head Attention\n",
    "The MultiHeadAttention class below invokes parrallel attention heads (for GPT2-Small) with each attention head learning different Q,K,V transformations to capture diverse patterns - like grammatical structure, semantic meaning, and position relationship between words. Each head computes scaled attention scores (scaled Q. K^T) to create weighted sums of values. The weighted sum of values represent a NEW, CONTEXT-AWARE representation of each token. Essentially, it's the token's original information enriched with relevant information from other tokens it should pay attention to. Casual masking is applied to these attention scores to prevent future token access, and all heads are concatenated to provide a rich, multi-perspective representation of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a1094fd-05da-45ee-ab09-34ef8076bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(\n",
    "            b, num_tokens, self.num_heads, self.head_dim\n",
    "        )\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3fc9c3-24be-4e55-ac88-d9f786089e50",
   "metadata": {},
   "source": [
    "### LayerNorm Class\n",
    "The LayerNorm class normalizes each token's embedding to have mean=0 and variance=1. Then it applies learned scale and shift parameters. This stablizes training by preventing internal covariate shift - ensuring that values don't become too large or too small as they pass through many layers; similar to how standardizing exam scores makes theme easier to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd36fe51-9033-4546-9058-1e0e41798af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb287d4-a4cf-4de9-b9d2-3e3224e808ff",
   "metadata": {},
   "source": [
    "### GELU Activation\n",
    "Activation functions are critical in deep learning networks as they add non-linearity.GELU stands for Gaussian Error Linear Unit. It is a smooth activation function that applies f(x) = x * Φ(x) where Φ is the cumulative distribution function of the standard normal distribution. It's similar to ReLU but smoother(no sharp corner at zero), allowing small negative values to pass through with reduced magnitude which helps gradients flow better during training - it's a softer gate that does not kill negative values completely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e2d49c-4286-422f-8193-f435c50f0960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a3a746-ec89-4d39-a1fa-e6bdb4e9f90e",
   "metadata": {},
   "source": [
    "### FeedForward Class\n",
    "The FeedForward class is a simple two-layer neural network that processes each token independently; it expands the embedding dimension by 4. So that means if the input tokens have a dimension of 768, it is multipled by 4 to give 3072. Subsequently GELU is applied for non-linearity activation, and then it is projected back to 768 dimention. This essentially gives each token a chance to \"think\" about it's contextualized representation from attention, transforming the features through a bottlenceck that learns complex patterns. It is almost like compressing and decompressing information to extract richer features from the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ed742b9-3bdb-4129-8e3e-1a14bc769cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c57062-efdc-42ab-9c6c-9a1e0b1292ab",
   "metadata": {},
   "source": [
    "### GPT Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e6ddf2-6327-421a-b455-cc6186ab9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 Small (124M parameters)\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,        # Number of tokens in vocabulary\n",
    "    \"context_length\": 1024,     # Maximum sequence length\n",
    "    \"emb_dim\": 768,             # Embedding dimension\n",
    "    \"n_heads\": 12,              # Number of attention heads\n",
    "    \"n_layers\": 12,             # Number of transformer blocks\n",
    "    \"drop_rate\": 0.1,           # Dropout probability\n",
    "    \"qkv_bias\": False           # Use bias in Q, K, V projections?\n",
    "}\n",
    "\n",
    "# GPT-2 Medium (355M parameters)\n",
    "GPT_CONFIG_355M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 1024,            # Larger embedding\n",
    "    \"n_heads\": 16,              # More heads\n",
    "    \"n_layers\": 24,             # More layers\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "# For testing (tiny model)\n",
    "GPT_CONFIG_TINY = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 256,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 4,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf1aa1-591f-4414-b50b-4b487bc6b40c",
   "metadata": {},
   "source": [
    "### TransformerBlock Class - The result of years of deep learning research\n",
    "\n",
    "Each token passes through two stages: (1) Multi-Head Attention gathers context from previous tokens (e.g. \"sat\" learns about \"cat\"), and (2) Feed-Forward processes this information through a neural network. The critical innovation is Residual connections symbolized by the shortcut variable below. This is where we save the input before each stage and add it back (x = x + shortcut), which prevents vanishing gradients and preserves each token's identity while enriching it with extra context.\n",
    "\n",
    "Layer Normalization before each stage keep values stable by normalizing to mean = 0 and variance = 1, preventing explosion or vanishing. Dropout has been set in the above configurations to 0.1 which means 10% random zeroing during training helps prevent overfitting by forcing the neural network to not rely on specific neurons. The overall pattern - normalize, transform, add residual-repeated for attention and feed-forward creates contextualized representations: for example, \"cat\" remains \"cat\" but is now enriched with \"subject who is sitting\". Stack 12 of these transformer blocks and you get GPT2-Small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb217a63-7d51-4179-9c63-82744f657090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad759581-c776-47f4-87de-1fbf9e666479",
   "metadata": {},
   "source": [
    "### GPTModel Class - The Complete Architecture\n",
    "\n",
    "Takes token IDs as input and outputs probability distributions over the vocabulary for predicting the next tokens. The flow: (1) Convert token IDs to embeddings and add positional information, (2) pass through a stack of 12 transformer blocks (declared above) where each token gathers context (learns more about itself) and processes this information, (3) apply final normalization and project back to vocabulary size to get logits (raw scores) for each possible next token.\n",
    "\n",
    "The key insight here is that by stacking multiple transformer blocks, each token's representation becomes progressively richer - early layers learn simple patterns like nearby words, middle layers learn grammar and syntax, and deep layers learn complex semantics and reasoning. The output logits can be converted to probabilities with softmax, then sampled to generate the next token, repeating this process to generate entire lengthy sequences of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c9422d6-f467-40db-ae1e-849974a0d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37b5b7-5128-482c-95fe-cf3306a1e9a3",
   "metadata": {},
   "source": [
    "### Generating Outputs using the GPTModel\n",
    "\n",
    "The generate_text_simple function generates text autoregressively (looking backwards) by repeatedly feeding the current sequence to the model, taking the highest-probability next token from the output logits, and appending it to the sequence. This process repeats for max_new_tokens iterations, building up the generated text one token at a time by always selecting the most likely continutation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a45922ec-c48a-4026-9161-11f49d328da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx,max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dcd3ff-8ade-4aac-a980-aa7ae8b22ab4",
   "metadata": {},
   "source": [
    "### Encoding, decoding and gibberish outputs\n",
    "\n",
    "The text_to_token_ids function converts a text string into token IDs using the tokenizer's vocabulary, and then wraps the result in a PyTorch tensor with a batch dimension so it can be fed into the model, which expects input shape of (batch_size, sequence length)\n",
    "\n",
    "On the other hand, the token_ids_to_text converts token IDs back to readable text by removing the batch dimension (done through the call to squeeze) and decoding the token ID list back to a string using the tokenizer's vocabulary\n",
    "\n",
    "Models need batch dimension due to parallelization so it can process multiple sentences at once with a GPU. The unsqueeze function call changes the text \"Hello\" from sequence length (2,) to (batch, sequence length) of (1,2). This also helps in matrix operations working correctly. The squeeze function removes the batch dimension because the tokenizer expects list of IDs, and those are simpler to work with. unsqueeze(0) adds a batch dimension so PyTorch is happy, squeeze(0) removes it so the tokenizer is happy!\n",
    "\n",
    "The output of the prompt below is gibberish because the model has random, untrained weights - it was jut initialized with GPTModel(GPT_CONFIG_124M) but never trained or loaded with pre-trained weights, so it has no knowledge of language. To get the coherent text either you (1) load pre-trained weights that most frontier model companies have publicly available, or (2) train the model on text data, which is known as PRE-TRAINING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12a04676-60ed-4f55-acce-0835a6307e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " What is the meaning of life? Times steroids OPSrem103 LIC superior wherein Tycooningly\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1456)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"What is the meaning of life?\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8908c7-dbd6-4eb0-9110-23605966c91f",
   "metadata": {},
   "source": [
    "### Pre-training the model on the Gutenberg dataset\n",
    "\n",
    "The cell below imports 50 book titles from the Gutenberg dataset including Alice in Wonderland, A Tale of Two Cities, Sherlock Holmes, Pride and Prejudice amongst others. Subsequently, we read each book's content into memory, and concatenate them all with newlines between books to create one large corpus string called text_data for training. Then we split the text data into training (90%) and validation (10%) sets by calculating a split index at the 90% point - the model will learn from train_data and we will evaluate it's performance using val_data to detect overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc2cf30-ffee-49e2-9359-babd150dfe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def download_gutenberg_corpus(num_books=50):\n",
    "    os.makedirs(\"gutenberg_books\", exist_ok=True)\n",
    "    \n",
    "    book_ids = [\n",
    "        1342, 84, 1661, 11, 98, 2701, 1260, 174, 46, 345,\n",
    "        74, 76, 244, 1400, 100, 2600, 1497, 5200, 64317, 16328,\n",
    "        1952, 215, 1727, 1232, 2814, 145, 408, 1184, 205, 161,\n",
    "        236, 1080, 514, 158, 43, 2148, 1635, 1259, 996, 6130,\n",
    "        4300, 120, 219, 768, 1250, 271, 30254, 2097, 135, 2500,\n",
    "        31284, 209, 863, 1298, 829, 308, 1404, 3825, 730, 1155,\n",
    "        1250, 521, 1998, 203, 1322, 1984, 844, 2591, 699, 2265,\n",
    "        140, 25344, 375, 45, 19337, 113, 1399, 2554, 1946, 1257,\n",
    "        19942, 6593, 160, 41, 1028, 2542, 16389, 526, 829, 1304,\n",
    "        3090, 580, 2500, 15399, 2160, 1998, 1184, 3207, 996\n",
    "    ][:num_books]\n",
    "    \n",
    "    print(f\"Downloading {len(book_ids)} books from Project Gutenberg...\")\n",
    "    print(\"This may take a few minutes...\\n\")\n",
    "    \n",
    "    successful = 0\n",
    "    for i, book_id in enumerate(book_ids):\n",
    "        filepath = os.path.join(\"gutenberg_books\", f\"pg{book_id}.txt\")\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"[{i+1}/{len(book_ids)}] ✓ Already exists: pg{book_id}.txt\")\n",
    "            successful += 1\n",
    "            continue\n",
    "        \n",
    "        url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"[{i+1}/{len(book_ids)}] Downloading book {book_id}...\", end=\" \")\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            \n",
    "            print(\"✓\")\n",
    "            successful += 1\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except requests.exceptions.HTTPError:\n",
    "            alt_url = f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt\"\n",
    "            try:\n",
    "                print(f\"trying alternate URL...\", end=\" \")\n",
    "                response = requests.get(alt_url, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "                \n",
    "                print(\"✓\")\n",
    "                successful += 1\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ Download complete!\")\n",
    "    print(f\"  Successfully downloaded: {successful}/{len(book_ids)} books\")\n",
    "    print(f\"  Location: ./gutenberg_books/\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return successful\n",
    "\n",
    "download_gutenberg_corpus(num_books=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "223d5a24-2156-405f-a555-92b1ede36e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 books\n",
      "Loaded 50 books\n",
      "Total characters: 42,475,753\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob \n",
    "import random\n",
    "\n",
    "gutenberg_path = \"gutenberg_books\"\n",
    "\n",
    "txt_files = glob.glob(os.path.join(gutenberg_path, \"*.txt\"))\n",
    "print(f\"Found {len(txt_files)} books\")\n",
    "\n",
    "random.seed(42)\n",
    "selected_files = random.sample(txt_files, min(50, len(txt_files)))\n",
    "\n",
    "all_texts = []\n",
    "for filepath in selected_files:\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        all_texts.append(f.read())\n",
    "\n",
    "text_data = \"\\n\\n\".join(all_texts)\n",
    "\n",
    "print(f\"Loaded {len(all_texts)} books\")\n",
    "print(f\"Total characters: {len(text_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7899f58b-63ed-4e51-abb5-99678be14e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 42475753\n",
      "Tokens: 11798600\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01018139-6521-4615-b1a3-31d7a429e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f0e05c-1591-4198-a612-c7b274afc1e3",
   "metadata": {},
   "source": [
    "### GPU Availability Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05c6cd29-d1d5-4068-b478-38bacb08cafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GB10\n",
      "✓ GPU is ready!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"✓ GPU is ready!\")\n",
    "else:\n",
    "    print(\"✗ Still CPU only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d97c8a5-8694-4ddf-9634-84a5aa66a512",
   "metadata": {},
   "source": [
    "### GPTDataset Class\n",
    "\n",
    "The GPTDataset class converts raw text into training examples by tokenizing it and creating sliding windows of input-target pairs—each input is a sequence of tokens, and the target is the same sequence shifted by one position (predicting the next token). For example, if input is [15496, 11, 995], the target is [11, 995, 0], teaching the model to predict each next token in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd268294-187d-44bd-82b9-4fc06c22aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17887e-e990-4e92-be7a-1b8f0fa64447",
   "metadata": {},
   "source": [
    "### DataLoader Function\n",
    "\n",
    "The DataLoader function creates a data loader by converting text into a GPTDatasetV1 (which generates input-target pairs), then wraps it in PyTorch's DataLOader to handle batching (grouping multiple examples together), shuffling (randomizing order for better training). and iteration - this allows you to loop through \"for input_batch, target_batch in dataloader\" during training with each batch containing batch_size (in this case 4) examples ready for the GPU.\n",
    "\n",
    "After this, we create two separate DataLoaders - one for training data (shuffled to prevent the model from memorizing the order) and one for validation data (not shuffled for consistent evaluation) with batch_size = 2 meaning each iteration provides 2 sequences at once, and stride = context_length (1024) ensuring no overlap between training windows to use data efficiently without reptition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "794ccde6-80b4-4bd5-b606-b0a347fff340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0xf77a60c5a8a0>\n",
      "Train loader: 5211 batches\n",
      "Val loader: 550 batches\n"
     ]
    }
   ],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print (val_loader)\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33295ffc-03df-40b3-9108-cfc6b1015753",
   "metadata": {},
   "source": [
    "### Loss Calculation Functions\n",
    "\n",
    "calc_loss_batch moves input and target tensors to GPU, runs them through the model to get predictions (logits), and computes cross-entropy loss (measuring how wrong the predictions are). calc_loss_loader evaluates the model on multiple batches from a dataloader and returns the average loss - useful for checking training/validaton performance without training, limiting to num_batches for faster evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fdf22b8-3132-4705-911f-c199ccb158d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bec560-bc8a-49f6-95f6-5b0876faf065",
   "metadata": {},
   "source": [
    "### Initialize Model on GPU and check initial loss\n",
    "\n",
    "Sets device to \"CUDA\" for GPU training, creates a fresh GPTModel with random weights, moves it to GPU and calculates initial losses on 5 batches to establish a baseline. Untrained models typically have loss around 10-11 which should decrease dramatically during training as the model learns patters in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7be586e8-8c1f-410c-84a1-8f5dc4753bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GB10\n",
      "============================================================\n",
      "\n",
      "Model parameters: 163,009,536\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mModel parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel.parameters())\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     train_loss = \u001b[43mcalc_loss_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     val_loss = calc_loss_loader(val_loader, model, device, num_batches=\u001b[32m5\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInitial losses (untrained model):\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mcalc_loss_loader\u001b[39m\u001b[34m(data_loader, model, device, num_batches)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (input_batch, target_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i < num_batches:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         loss = \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m         total_loss += loss.item()\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mcalc_loss_batch\u001b[39m\u001b[34m(input_batch, target_batch, model, device)\u001b[39m\n\u001b[32m      2\u001b[39m input_batch = input_batch.to(device)\n\u001b[32m      3\u001b[39m target_batch = target_batch.to(device)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m loss = torch.nn.functional.cross_entropy(\n\u001b[32m      6\u001b[39m     logits.flatten(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m), target_batch.flatten()\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterlab/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterlab/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, in_idx)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_idx):\n\u001b[32m     17\u001b[39m     batch_size, seq_len = in_idx.shape\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     tok_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtok_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     pos_embeds = \u001b[38;5;28mself\u001b[39m.pos_emb(\n\u001b[32m     21\u001b[39m         torch.arange(seq_len, device=in_idx.device)\n\u001b[32m     22\u001b[39m     )\n\u001b[32m     23\u001b[39m     x = tok_embeds + pos_embeds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterlab/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterlab/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterlab/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jupyterlab/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"\\nInitial losses (untrained model):\")\n",
    "print(f\"  Training loss: {train_loss:.3f}\")\n",
    "print(f\"  Validation loss: {val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ca7f4-8348-4e2a-a4ab-4c7b4c975a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
